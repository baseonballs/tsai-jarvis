#!/usr/bin/env python3
"""
Real Hockey Data Integration - Complete pipeline for real hockey data
"""

import os
import sys
import json
import time
import logging
import asyncio
from pathlib import Path
from typing import Dict, List, Any, Optional
from datetime import datetime

# Add data collection modules to path
sys.path.append('/Volumes/Thorage/wip/tsai-jarvis/phase1/data_collection')
from hockey_data_collector import HockeyDataCollector
from data_preprocessor import HockeyDataPreprocessor

# Add phase1 to path
sys.path.append('/Volumes/Thorage/wip/tsai-jarvis/phase1')
from hockey_role_classifier_standalone import HockeyRoleClassificationPipeline

class RealHockeyDataIntegration:
    """Complete pipeline for integrating real hockey data"""
    
    def __init__(self, data_dir: str = "/data/hockey_players"):
        self.data_dir = Path(data_dir)
        self.logger = logging.getLogger("RealHockeyDataIntegration")
        self.setup_logging()
        
        # Initialize components
        self.data_collector = HockeyDataCollector(str(self.data_dir))
        self.data_preprocessor = HockeyDataPreprocessor(str(self.data_dir))
        
        # Integration configuration
        self.config = {
            'target_accuracy': 0.85,
            'data_config': {
                'dataset_path': str(self.data_dir),
                'train_split': 0.8,
                'val_split': 0.1,
                'test_split': 0.1
            },
            'training_config': {
                'epochs': 100,
                'batch_size': 32,
                'learning_rate': 0.001
            },
            'device': 'cpu'
        }
        
    def setup_logging(self):
        """Setup logging configuration"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
    
    def collect_real_data(self) -> Dict[str, Any]:
        """Collect real hockey data from various sources"""
        try:
            self.logger.info("ğŸ“¥ Collecting real hockey data...")
            
            # Run data collection
            collection_metadata = self.data_collector.run_data_collection()
            
            if not collection_metadata:
                self.logger.error("âŒ Data collection failed")
                return {}
            
            self.logger.info("âœ… Real hockey data collection completed")
            return collection_metadata
            
        except Exception as e:
            self.logger.error(f"âŒ Real data collection failed: {e}")
            return {}
    
    def preprocess_real_data(self) -> Dict[str, Any]:
        """Preprocess collected real hockey data"""
        try:
            self.logger.info("ğŸ”§ Preprocessing real hockey data...")
            
            # Run data preprocessing
            preprocessing_results = self.data_preprocessor.process_all_data()
            
            if not preprocessing_results:
                self.logger.error("âŒ Data preprocessing failed")
                return {}
            
            # Generate preprocessing report
            report = self.data_preprocessor.generate_dataset_report(preprocessing_results)
            
            self.logger.info("âœ… Real hockey data preprocessing completed")
            return report
            
        except Exception as e:
            self.logger.error(f"âŒ Real data preprocessing failed: {e}")
            return {}
    
    def validate_dataset(self) -> Dict[str, Any]:
        """Validate the prepared dataset"""
        try:
            self.logger.info("âœ… Validating prepared dataset...")
            
            validation_results = {
                'total_samples': 0,
                'role_distribution': {},
                'split_distribution': {},
                'data_quality': {},
                'validation_passed': True
            }
            
            roles = ['player', 'goalie', 'referee']
            splits = ['train', 'val', 'test']
            
            # Check each role and split
            for role in roles:
                role_samples = 0
                for split in splits:
                    split_dir = self.data_dir / split / role
                    if split_dir.exists():
                        samples = len(list(split_dir.glob('*.jpg')))
                        role_samples += samples
                        validation_results['split_distribution'][f"{split}_{role}"] = samples
                    else:
                        self.logger.warning(f"âš ï¸ Missing directory: {split_dir}")
                        validation_results['validation_passed'] = False
                
                validation_results['role_distribution'][role] = role_samples
                validation_results['total_samples'] += role_samples
            
            # Check minimum data requirements
            min_samples_per_role = 50
            min_samples_per_split = 10
            
            for role in roles:
                if validation_results['role_distribution'][role] < min_samples_per_role:
                    self.logger.warning(f"âš ï¸ Insufficient samples for {role}: {validation_results['role_distribution'][role]}")
                    validation_results['validation_passed'] = False
            
            # Data quality checks
            validation_results['data_quality'] = {
                'min_samples_per_role': min_samples_per_role,
                'min_samples_per_split': min_samples_per_split,
                'total_roles': len(roles),
                'total_splits': len(splits)
            }
            
            self.logger.info(f"ğŸ“Š Dataset validation results:")
            self.logger.info(f"  - Total samples: {validation_results['total_samples']}")
            self.logger.info(f"  - Role distribution: {validation_results['role_distribution']}")
            self.logger.info(f"  - Validation passed: {validation_results['validation_passed']}")
            
            return validation_results
            
        except Exception as e:
            self.logger.error(f"âŒ Dataset validation failed: {e}")
            return {'validation_passed': False}
    
    def train_with_real_data(self) -> Dict[str, Any]:
        """Train the model with real hockey data"""
        try:
            self.logger.info("ğŸ‹ï¸ Training model with real hockey data...")
            
            # Create training pipeline
            pipeline = HockeyRoleClassificationPipeline(self.config)
            
            # Run complete training pipeline
            success = pipeline.run_pipeline()
            
            if success:
                self.logger.info("âœ… Model training with real data completed successfully")
                return {
                    'training_success': True,
                    'target_accuracy_achieved': True,
                    'model_performance': 'excellent'
                }
            else:
                self.logger.warning("âš ï¸ Model training completed but target accuracy not achieved")
                return {
                    'training_success': True,
                    'target_accuracy_achieved': False,
                    'model_performance': 'needs_improvement'
                }
            
        except Exception as e:
            self.logger.error(f"âŒ Model training with real data failed: {e}")
            return {
                'training_success': False,
                'target_accuracy_achieved': False,
                'model_performance': 'failed'
            }
    
    def run_complete_integration(self) -> Dict[str, Any]:
        """Run complete real hockey data integration pipeline"""
        try:
            self.logger.info("ğŸš€ Starting complete real hockey data integration...")
            
            integration_results = {
                'pipeline_start': datetime.now().isoformat(),
                'steps_completed': [],
                'overall_success': False
            }
            
            # Step 1: Collect real data
            self.logger.info("ğŸ“¥ Step 1: Collecting real hockey data...")
            collection_results = self.collect_real_data()
            if collection_results:
                integration_results['steps_completed'].append('data_collection')
                integration_results['collection_results'] = collection_results
            else:
                self.logger.error("âŒ Data collection failed")
                return integration_results
            
            # Step 2: Preprocess data
            self.logger.info("ğŸ”§ Step 2: Preprocessing real hockey data...")
            preprocessing_results = self.preprocess_real_data()
            if preprocessing_results:
                integration_results['steps_completed'].append('data_preprocessing')
                integration_results['preprocessing_results'] = preprocessing_results
            else:
                self.logger.error("âŒ Data preprocessing failed")
                return integration_results
            
            # Step 3: Validate dataset
            self.logger.info("âœ… Step 3: Validating prepared dataset...")
            validation_results = self.validate_dataset()
            if validation_results.get('validation_passed', False):
                integration_results['steps_completed'].append('dataset_validation')
                integration_results['validation_results'] = validation_results
            else:
                self.logger.error("âŒ Dataset validation failed")
                return integration_results
            
            # Step 4: Train model with real data
            self.logger.info("ğŸ‹ï¸ Step 4: Training model with real hockey data...")
            training_results = self.train_with_real_data()
            if training_results.get('training_success', False):
                integration_results['steps_completed'].append('model_training')
                integration_results['training_results'] = training_results
            else:
                self.logger.error("âŒ Model training failed")
                return integration_results
            
            # Step 5: Generate integration report
            self.logger.info("ğŸ“„ Step 5: Generating integration report...")
            integration_results['pipeline_end'] = datetime.now().isoformat()
            integration_results['overall_success'] = True
            
            # Save integration report
            report_path = self.data_dir / 'metadata' / 'integration_report.json'
            with open(report_path, 'w') as f:
                json.dump(integration_results, f, indent=2)
            
            self.logger.info("ğŸ‰ Complete real hockey data integration successful!")
            return integration_results
            
        except Exception as e:
            self.logger.error(f"âŒ Real hockey data integration failed: {e}")
            return {
                'pipeline_start': datetime.now().isoformat(),
                'pipeline_end': datetime.now().isoformat(),
                'steps_completed': [],
                'overall_success': False,
                'error': str(e)
            }
    
    def create_sample_data(self) -> bool:
        """Create sample real hockey data for testing"""
        try:
            self.logger.info("ğŸ“ Creating sample real hockey data...")
            
            # Create sample data structure
            sample_data = {
                'player': [
                    {'filename': 'player_001.jpg', 'description': 'Hockey player in action'},
                    {'filename': 'player_002.jpg', 'description': 'Player with stick'},
                    {'filename': 'player_003.jpg', 'description': 'Player in uniform'}
                ],
                'goalie': [
                    {'filename': 'goalie_001.jpg', 'description': 'Goalie making save'},
                    {'filename': 'goalie_002.jpg', 'description': 'Goalie in net'},
                    {'filename': 'goalie_003.jpg', 'description': 'Goalie with equipment'}
                ],
                'referee': [
                    {'filename': 'referee_001.jpg', 'description': 'Referee calling penalty'},
                    {'filename': 'referee_002.jpg', 'description': 'Referee on ice'},
                    {'filename': 'referee_003.jpg', 'description': 'Referee with whistle'}
                ]
            }
            
            # Create sample images
            for role, images in sample_data.items():
                for split in ['train', 'val', 'test']:
                    split_dir = self.data_dir / 'raw' / role  # Put all in raw/role, not raw/split/role
                    split_dir.mkdir(parents=True, exist_ok=True)
                    
                    for img_info in images:
                        img_path = split_dir / img_info['filename']
                        
                        # Create sample image (in real implementation, use actual hockey images)
                        import numpy as np
                        sample_image = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)
                        
                        # Add some role-specific characteristics (mock)
                        if role == 'goalie':
                            sample_image[:, :, 0] = np.minimum(sample_image[:, :, 0] + 50, 255)  # More blue
                        elif role == 'referee':
                            sample_image[:, :, 2] = np.minimum(sample_image[:, :, 2] + 50, 255)  # More red
                        
                        import cv2
                        cv2.imwrite(str(img_path), sample_image)
            
            self.logger.info("âœ… Sample real hockey data created")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Sample data creation failed: {e}")
            return False

def main():
    """Main function for real hockey data integration"""
    integration = RealHockeyDataIntegration()
    
    # Create sample data first
    if integration.create_sample_data():
        print("âœ… Sample data created successfully")
    else:
        print("âŒ Sample data creation failed")
        return
    
    # Run complete integration
    results = integration.run_complete_integration()
    
    if results.get('overall_success', False):
        print("ğŸ‰ Real hockey data integration completed successfully!")
        print(f"ğŸ“Š Steps completed: {results['steps_completed']}")
        print(f"ğŸ“ˆ Collection results: {results.get('collection_results', {}).get('collection_info', {})}")
        print(f"ğŸ”§ Preprocessing results: {results.get('preprocessing_results', {}).get('data_quality', {})}")
        print(f"âœ… Validation results: {results.get('validation_results', {}).get('validation_passed', False)}")
        print(f"ğŸ‹ï¸ Training results: {results.get('training_results', {}).get('training_success', False)}")
    else:
        print("âŒ Real hockey data integration failed!")
        print(f"ğŸ“‹ Steps completed: {results.get('steps_completed', [])}")
        if 'error' in results:
            print(f"âŒ Error: {results['error']}")

if __name__ == "__main__":
    main()
